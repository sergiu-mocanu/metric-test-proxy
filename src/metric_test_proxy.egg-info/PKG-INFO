Metadata-Version: 2.4
Name: metric-test-proxy
Version: 0.1.2
Summary: CLI for code similarity experiments: using textual similarity metrics as AI-code test pre-filtering mechanism.
Author-email: Sergiu Mocanu <sergiu.mocanu10@outlook.com>
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: evaluate
Requires-Dist: datasets>=3.6.0
Requires-Dist: rouge-score
Requires-Dist: sacrebleu
Requires-Dist: codebleu
Requires-Dist: tree-sitter-python>=0.21
Requires-Dist: crystalbleu
Requires-Dist: numpy
Requires-Dist: matplotlib
Requires-Dist: seaborn
Requires-Dist: scikit-learn
Requires-Dist: nltk>=3.9.1
Requires-Dist: typer

# Textual Metrics as a Test Proxy

## Validate metric scores obtained in summer 2024

This guide will allow to easily install and re-execute the *Textual Metric Measurement* on the `HumanEval+` benchmark.

Side note: due to the long duration of the full experimental protocol (> 2 days), the validation is done on a small
subset of the benchmark: code generated by `chatgpt_temp_0.0`.

## Install guide

### 1. Install Python
Make sure Python 3.9 or higher is installed on your system:

#### Linux:
```angular2html
sudo apt update
sudo apt install python3 python3-pip
```

#### macOS:
```angular2html
brew install python
```

### 2. Install pipx
Install `pipx` using Pythonâ€™s package manager. If your system prevents this by default, add the `--break-system-packages` 
flag as needed:

```angular2html
python3 -m pip install --user pipx
```
or
```angular2html
python3 -m pip install --break-system-packages --user pipx
```

Ensure that `pipx` was successfully installed and added to PATH:
```angular2html
~/.local/bin/pipx --version
export PATH="$HOME/.local/bin:$PATH"
source ~/.bashrc
pipx ensurepath
```


### 3. Build the project
`pipx install .` or `pipx install <path_to_project>`

Any changes to the project require a re-build: `pipx install --force .`



### 4. Extract validation set
Extract the validation dataset __textual-metrics.zip__ to `/home/username`


### 5. Run the script
In the console, run the following commands:

`run-metric-exp`: execute the benchmark on the extracted validation subset

`run-metric-validation`: compare the results with those obtained in summer of 2024

`delete-metric-results`: delete the newly obtained results before re-executing the benchmark


### 6. (Optional) Delete the project
In order to delete the `pipx` project and all the generated CLI entrypoints: 

`pipx uninstall metric_score_validation`


## Important note
When repeating the experimental protocol, it is noticeable that the results of `bleu` and `codebleu` metrics may differ 
from one execution to another, while other metrics give the same scores.
