[project]
name = "metric_score_validation"
version = "0.1.2"
description = "Validate the similarity of obtained metric score on the same dataset. Validation required due to a long delay since the last execution of the experiment (i.e., make sure that the employed metric libraries did not change behavior)."
authors = [
    { name = "Sergiu Mocanu", email = "sergiu.mocanu@inria.fr" }
]
dependencies = [
	"pandas",
	"evaluate",
	"datasets>=3.6.0",
	"rouge-score",
	"sacrebleu",
	"codebleu",
	"tree-sitter-python==0.21"
]
requires-python = ">=3.8"

[project.scripts]
run-metric-exp = "metric_measurement.textual_metrics:run_full_exp"
run-metric-validation = "metric_measurement.textual_metrics:run_metric_validation"
delete-metric-results = "metric_measurement.textual_metrics:delete_metric_results"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["metric_measurement"]

